{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "6n5_Q-Ts4HET"
      },
      "outputs": [],
      "source": [
        "!pip install langchain -q\n",
        "!pip install google-generativeai -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import google.generativeai as genai\n",
        "from langchain.llms.base import LLM\n"
      ],
      "metadata": {
        "id": "K5asLrScFUXH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Fetching API key directly from Colab userdata\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Check if the API key is present\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"API key not found. Make sure 'GOOGLE_API_KEY' is set in user data.\")\n"
      ],
      "metadata": {
        "id": "iq7_e29EFZn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the API with the provided key\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n"
      ],
      "metadata": {
        "id": "WU2P8ILXFfH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeminiLLM(LLM):\n",
        "    model = \"gemini-pro\"\n",
        "    temperature = 0.7\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"google_gemini\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        return genai.GenerativeModel(self.model).generate_content(prompt, generation_config={\"temperature\": self.temperature}).text\n"
      ],
      "metadata": {
        "id": "4b_pQIy8F022"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LLM with default model (\"gemini-pro\") and temperature (0.7) set in the class\n",
        "llm = GeminiLLM()\n",
        "\n"
      ],
      "metadata": {
        "id": "dwv4jQPfGHHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a prompt template for asking questions\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],  # Variable to take user input\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "ChVKZPIdGmsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the LangChain pipeline that combines the LLM with the prompt template\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n"
      ],
      "metadata": {
        "id": "J3Ch1QAeGs0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample question for the model\n",
        "question = \"What is LangChain?\"\n",
        "\n",
        "# Get the response by running the chain with the question\n",
        "response = chain.run({\"question\": question})\n",
        "\n",
        "# Output the result\n",
        "print(\"Answer:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "i3kgm3MVGvUP",
        "outputId": "157a74ef-19e8-4ef6-e292-7d29aa4b538f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
           "Answer: LangChain is a large language model from Google AI. It is trained on a massive dataset of text and code, and it can perform a variety of natural language processing tasks, such as:\n",
            "\n",
            "* Text generation\n",
            "* Machine translation\n",
            "* Question answering\n",
            "* Summarization\n",
            "* Chatbots\n",
            "\n",
            "LangChain is one of the most powerful language models available, and it is used in a wide variety of applications, such as:\n",
            "\n",
            "* Search engines\n",
            "* Chatbots\n",
            "* Language translation services\n",
            "* Text editors\n",
            "* Summarization tools\n",
            "\n",
            "LangChain is still under development, but it has already shown great promise for a variety of natural language processing tasks. As it continues to improve, it is likely to have an even greater impact on the way we interact with computers.\n"
          ]
        }
      ]
    }
  ]
}
